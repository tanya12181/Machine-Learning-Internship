{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing for major project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGPcAgaltq1AMjk5SXUFug",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanya12181/Machine-Learning-Internship-at-Internity/blob/main/Preprocessing_for_major_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibcYPae1vgKj"
      },
      "source": [
        "\"\"\"\r\n",
        "The fit_on_texts method is a part of Keras tokenizer class which is used to update \r\n",
        "the internal vocabulary for the texts list. \r\n",
        "We need to call be before using other methods of texts_to_sequences or texts_to_matrix.\r\n",
        "\r\n",
        "fit_on_texts.word_index assigns a unique index for each word.\r\n",
        "\"\"\"\r\n",
        "import json \r\n",
        "import re\r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "\r\n",
        "def pad_sequences(sequences,maxlen,padding = 'pre',value=0):\r\n",
        "  padding_sequences = sequences\r\n",
        "  if padding=='pre':\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.insert(0,value)\r\n",
        "  else:\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.append(value)\r\n",
        "  return np.array(padding_sequences)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def texts_to_sequences(training_sentences,word_index):\r\n",
        "  seq=[]\r\n",
        "  bad_char = [',','?','/','_']\r\n",
        "  for sentence in training_sentences:\r\n",
        "    w=[]\r\n",
        "    for c in bad_char:\r\n",
        "      sentence = sentence.replace(c,'')\r\n",
        "    for word in sentence.split():\r\n",
        "      word = word.lower()\r\n",
        "      w.append(word_index[word])\r\n",
        "    seq.append(w)\r\n",
        "  return seq\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def Label_Encoder(training_labels):\r\n",
        "  class_array = []\r\n",
        "  class_labels = []\r\n",
        "  idx = 0\r\n",
        "  for data in training_labels:\r\n",
        "    if data not in class_array:\r\n",
        "      class_array.append(data)\r\n",
        "      class_labels.append(idx)\r\n",
        "      idx+=1\r\n",
        "  num_array = []\r\n",
        "  class_array = sorted(class_array)\r\n",
        "  d = {class_array[i]:class_labels[i] for i in range(len(class_array))}\r\n",
        "  for label in training_labels:\r\n",
        "    num_array.append(d[label])\r\n",
        "  return(np.array(num_array))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def inverse_transform(similarity_vector,training_labels):\r\n",
        "  class_arr = []\r\n",
        "  for cls in training_labels:\r\n",
        "    if cls not in class_arr:\r\n",
        "      class_arr.append(cls)\r\n",
        "  max_index = None\r\n",
        "  max = 0\r\n",
        "  for index in range(len(similarity_vector)):\r\n",
        "    if max<similarity_vector[index]:\r\n",
        "      max_index = index\r\n",
        "      max = similarity_vector[index]\r\n",
        "  return class_arr[max_index]\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "#print(word)\r\n",
        "#print(sentence)\r\n",
        "#print(sentence)\r\n",
        "#sentence = re.match(\"[a-zA-Z]+\",sentence).group(0)\r\n",
        "#sentence = re.sub(r\"\\W+\",\" \", sentence)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-J-q5NeurV"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords', quiet = True)\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "stop_words = list(stopwords.words('english'))\r\n",
        "#stop_words\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHv4tc74fVac"
      },
      "source": [
        "def remove_stopwords(stop_words,training_sentences):\r\n",
        "  new_sentences = []\r\n",
        "  for sentence in training_sentences:\r\n",
        "    s = ''\r\n",
        "    words = sentence.split()\r\n",
        "    for word in words:\r\n",
        "      if word not in stop_words:\r\n",
        "        s+=(word+' ')\r\n",
        "    s=s.strip()\r\n",
        "    new_sentences.append(s)\r\n",
        "  return new_sentences\r\n",
        "\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Ef2OAMccqI",
        "outputId": "125baa2f-f928-41a7-9f4e-5451f6241ec7"
      },
      "source": [
        "with open('intents.json') as file:\r\n",
        "    data = json.load(file)\r\n",
        "\r\n",
        "\r\n",
        "training_sentences = []\r\n",
        "training_labels = []\r\n",
        "labels = []\r\n",
        "responses = []\r\n",
        "\r\n",
        "\r\n",
        "for intent in data['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "        training_sentences.append(pattern)\r\n",
        "        training_labels.append(intent['tag'])\r\n",
        "    responses.append(intent['responses'])\r\n",
        "    \r\n",
        "    if intent['tag'] not in labels:\r\n",
        "        labels.append(intent['tag'])\r\n",
        "\r\n",
        "#print(training_labels)\r\n",
        "        \r\n",
        "num_classes = len(labels)\r\n",
        "\r\n",
        "lbl_encoder = Label_Encoder(training_labels)\r\n",
        "print(lbl_encoder)\r\n",
        "new_sentences = remove_stopwords(stop_words,training_sentences)\r\n",
        "print(\"training sentences : \",training_sentences)\r\n",
        "print(\"after removing stopwords:\")\r\n",
        "print(\"new sentences : \",new_sentences)\r\n",
        "\r\n",
        "vocab_size = 1000\r\n",
        "embedding_dim = 16\r\n",
        "max_len = 20\r\n",
        "oov_token = \"<OOV>\"\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) \r\n",
        "tokenizer.fit_on_texts(training_sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = texts_to_sequences(training_sentences,word_index)\r\n",
        "padded_sequences = pad_sequences(sequences,max_len)\r\n",
        "\r\n",
        "epochs = 550\r\n",
        "#history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)\r\n",
        "\r\n",
        "#print(lbl_encoder.fit_transform(training_labels))\r\n",
        "#lbl_encoder.fit(training_labels)\r\n",
        "#training_labels = lbl_encoder.transform(training_labels)\r\n",
        "\r\n",
        "#padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\r\n",
        "#tokenizer.texts_to_sequences(training_sentences)\r\n",
        "#print(labels,end= \"\\n\\n\\n\")\r\n",
        "#print(training_sentences,end=\"\\n\\n\\n\")\r\n",
        "#print(lbl_encoder,end='\\n\\n\\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 4 4 4 4 4 4 3 3 3 3 3 8 8 8 8 8 6 6 6 6 6 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2\n",
            " 7 7 7 7 7 5 5 5 5 5]\n",
            "training sentences :  ['Hi there', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'Bye', 'See you later', 'Goodbye', 'Nice chatting to you, bye', 'Till next time', 'Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me', 'How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?', 'What support is offered', 'How to check Adverse drug reaction?', 'Open adverse drugs module', 'Give me a list of drugs causing adverse behavior', 'List all drugs suitable for patient with adverse reaction', 'Which drugs dont have adverse reaction?', 'Open blood pressure module', 'Task related to blood pressure', 'Blood pressure data entry', 'I want to log blood pressure results', 'Blood pressure data management', 'I want to search for blood pressure result history', 'Blood pressure for patient', 'Load patient blood pressure result', 'Show blood pressure results for patient', 'Find blood pressure results by ID', 'Find me a pharmacy', 'Find pharmacy', 'List of pharmacies nearby', 'Locate pharmacy', 'Search pharmacy', 'Lookup for hospital', 'Searching for hospital to transfer patient', 'I want to search hospital data', 'Hospital lookup for patient', 'Looking up hospital details']\n",
            "after removing stopwords:\n",
            "new sentences :  ['Hi', 'How', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'Bye', 'See later', 'Goodbye', 'Nice chatting you, bye', 'Till next time', 'Thanks', 'Thank', \"That's helpful\", 'Awesome, thanks', 'Thanks helping', 'How could help me?', 'What do?', 'What help provide?', 'How helpful?', 'What support offered', 'How check Adverse drug reaction?', 'Open adverse drugs module', 'Give list drugs causing adverse behavior', 'List drugs suitable patient adverse reaction', 'Which drugs dont adverse reaction?', 'Open blood pressure module', 'Task related blood pressure', 'Blood pressure data entry', 'I want log blood pressure results', 'Blood pressure data management', 'I want search blood pressure result history', 'Blood pressure patient', 'Load patient blood pressure result', 'Show blood pressure results patient', 'Find blood pressure results ID', 'Find pharmacy', 'Find pharmacy', 'List pharmacies nearby', 'Locate pharmacy', 'Search pharmacy', 'Lookup hospital', 'Searching hospital transfer patient', 'I want search hospital data', 'Hospital lookup patient', 'Looking hospital details']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaRLOS-h4o0N"
      },
      "source": [
        "# print(type(padded_sequences))\r\n",
        "# print(padded_sequences[0,:].shape)\r\n",
        "# print(padded_sequences)\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58e5zc3l1Wsf",
        "outputId": "f82ccb37-7184-4ed8-fbcb-38506f9d2d35"
      },
      "source": [
        "print(word_index)\r\n",
        "word_index = dict(sorted(word_index.items()))\r\n",
        "#word_index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'blood': 2, 'pressure': 3, 'you': 4, 'for': 5, 'to': 6, 'patient': 7, 'adverse': 8, 'hospital': 9, 'how': 10, 'me': 11, 'drugs': 12, 'pharmacy': 13, 'thanks': 14, 'what': 15, 'reaction': 16, 'list': 17, 'data': 18, 'i': 19, 'want': 20, 'results': 21, 'search': 22, 'find': 23, 'there': 24, 'is': 25, 'bye': 26, 'helpful': 27, 'help': 28, 'can': 29, 'open': 30, 'module': 31, 'a': 32, 'of': 33, 'result': 34, 'lookup': 35, 'hi': 36, 'are': 37, 'anyone': 38, 'hey': 39, 'hola': 40, 'hello': 41, 'good': 42, 'day': 43, 'see': 44, 'later': 45, 'goodbye': 46, 'nice': 47, 'chatting': 48, 'till': 49, 'next': 50, 'time': 51, 'thank': 52, \"that's\": 53, 'awesome': 54, 'helping': 55, 'could': 56, 'do': 57, 'provide': 58, 'be': 59, 'support': 60, 'offered': 61, 'check': 62, 'drug': 63, 'give': 64, 'causing': 65, 'behavior': 66, 'all': 67, 'suitable': 68, 'with': 69, 'which': 70, 'dont': 71, 'have': 72, 'task': 73, 'related': 74, 'entry': 75, 'log': 76, 'management': 77, 'history': 78, 'load': 79, 'show': 80, 'by': 81, 'id': 82, 'pharmacies': 83, 'nearby': 84, 'locate': 85, 'searching': 86, 'transfer': 87, 'looking': 88, 'up': 89, 'details': 90}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab1GOA8-fah",
        "outputId": "665c9235-c342-485b-d233-bc9d7ebb3963"
      },
      "source": [
        "seq = texts_to_sequences(training_sentences,word_index)\r\n",
        "print(seq)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[36, 24], [10, 37, 4], [25, 38, 24], [39], [40], [41], [42, 43], [26], [44, 4, 45], [46], [47, 48, 6, 4, 26], [49, 50, 51], [14], [52, 4], [53, 27], [54, 14], [14, 5, 55, 11], [10, 4, 56, 28, 11], [15, 4, 29, 57], [15, 28, 4, 58], [10, 4, 29, 59, 27], [15, 60, 25, 61], [10, 6, 62, 8, 63, 16], [30, 8, 12, 31], [64, 11, 32, 17, 33, 12, 65, 8, 66], [17, 67, 12, 68, 5, 7, 69, 8, 16], [70, 12, 71, 72, 8, 16], [30, 2, 3, 31], [73, 74, 6, 2, 3], [2, 3, 18, 75], [19, 20, 6, 76, 2, 3, 21], [2, 3, 18, 77], [19, 20, 6, 22, 5, 2, 3, 34, 78], [2, 3, 5, 7], [79, 7, 2, 3, 34], [80, 2, 3, 21, 5, 7], [23, 2, 3, 21, 81, 82], [23, 11, 32, 13], [23, 13], [17, 33, 83, 84], [85, 13], [22, 13], [35, 5, 9], [86, 5, 9, 6, 87, 7], [19, 20, 6, 22, 9, 18], [9, 35, 5, 7], [88, 89, 9, 90]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDARORm9G_Wr",
        "outputId": "3e28f715-d62e-4dcd-845c-4fb45a20e9cf"
      },
      "source": [
        "print(padded_sequences)\r\n",
        "#padded_sequences2[0,:].shape\r\n",
        "# if seq == sequences:\r\n",
        "#   print('true')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 37  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25 38 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 39]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 40]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 41]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 42 43]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 44  4 45]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 47 48  6  4 26]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 49 50 51]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 52  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 53 27]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 54 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  5 55 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  4 56 28 11]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  4 29 57]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 28  4 58]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  4 29 59 27]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 60 25 61]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  6 62  8 63 16]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30  8 12 31]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 64 11 32 17 33 12 65  8 66]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 17 67 12 68  5  7 69  8 16]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 70 12 71 72  8 16]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 30  2  3 31]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 73 74  6  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 18 75]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 19 20  6 76  2  3 21]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 18 77]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0 19 20  6 22  5  2  3 34 78]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3  5  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 79  7  2  3 34]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 80  2  3 21  5  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  2  3 21 81 82]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23 11 32 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17 33 83 84]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 85 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 22 13]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35  5  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 86  5  9  6 87  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 19 20  6 22  9 18]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9 35  5  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 88 89  9 90]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0WCIRSZ7z7"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKNik1r9n6a"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMW5ZNXU9-kX"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWC20srd-3_T"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol3pdZ9D_KVf"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8p4cPJYCgQQ"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}

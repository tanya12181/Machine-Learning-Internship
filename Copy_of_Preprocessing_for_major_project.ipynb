{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Preprocessing for major project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO5sKOq7/sYVGg9TcDCDCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanya12181/Machine-Learning-Internship-at-Internity/blob/main/Copy_of_Preprocessing_for_major_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibcYPae1vgKj"
      },
      "source": [
        "\"\"\"\r\n",
        "The fit_on_texts method is a part of Keras tokenizer class which is used to update \r\n",
        "the internal vocabulary for the texts list. \r\n",
        "We need to call be before using other methods of texts_to_sequences or texts_to_matrix.\r\n",
        "\r\n",
        "fit_on_texts.word_index assigns a unique index for each word.\r\n",
        "\"\"\"\r\n",
        "import json \r\n",
        "import re\r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "\r\n",
        "def pad_sequences(sequences,maxlen,padding = 'pre',value=0):\r\n",
        "  padding_sequences = sequences\r\n",
        "  if padding=='pre':\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.insert(0,value)\r\n",
        "  else:\r\n",
        "    for seq in padding_sequences:\r\n",
        "      while(len(seq)<maxlen):\r\n",
        "        seq.append(value)\r\n",
        "  return np.array(padding_sequences)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def texts_to_sequences(training_sentences,word_index):\r\n",
        "  seq=[]\r\n",
        "  bad_char = [',','?','/','_','@','#','*']\r\n",
        "  for sentence in training_sentences:\r\n",
        "    w=[]\r\n",
        "    for c in bad_char:\r\n",
        "      sentence = sentence.replace(c,'')\r\n",
        "    for word in sentence.split():\r\n",
        "      word = word.lower()\r\n",
        "      w.append(word_index[word])\r\n",
        "    seq.append(w)\r\n",
        "  return seq\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class label_encoder:\r\n",
        "\r\n",
        "  def __init__(self,training_labels):\r\n",
        "    self.training_labels = training_labels\r\n",
        "    self.class_array = []\r\n",
        "    self.class_labels = []\r\n",
        "\r\n",
        "\r\n",
        "  def find_class_array(self):\r\n",
        "    self.class_array = []\r\n",
        "    self.class_labels = []\r\n",
        "    idx = 0\r\n",
        "    for data in self.training_labels:\r\n",
        "      if data not in self.class_array:\r\n",
        "        self.class_array.append(data)\r\n",
        "        self.class_labels.append(idx)\r\n",
        "        idx+=1\r\n",
        "    self.class_array = sorted(self.class_array)\r\n",
        "    #print(self.class_array)\r\n",
        "    \r\n",
        "\r\n",
        "  def Label_Encoder(self):\r\n",
        "    num_array = []\r\n",
        "    #print(self.class_array)\r\n",
        "    d = {self.class_array[i]:self.class_labels[i] for i in range(len(self.class_array))}\r\n",
        "    for label in self.training_labels:\r\n",
        "      num_array.append(d[label])\r\n",
        "    return(np.array(num_array))\r\n",
        "\r\n",
        "\r\n",
        "  def inverse_transform(similarity_vector,self):\r\n",
        "    max_index = None\r\n",
        "    max = 0\r\n",
        "    for index in range(len(similarity_vector)):\r\n",
        "      if max<similarity_vector[index]:\r\n",
        "        max_index = index\r\n",
        "        max = similarity_vector[index]\r\n",
        "    return self.class_array[max_index]\r\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-J-q5NeurV"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords', quiet = True)\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "stop_words = list(stopwords.words('english'))\r\n",
        "#stop_words\r\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHv4tc74fVac"
      },
      "source": [
        "def remove_stopwords(stop_words,training_sentences):\r\n",
        "  new_sentences = []\r\n",
        "  for sentence in training_sentences:\r\n",
        "    s = ''\r\n",
        "    words = sentence.split()\r\n",
        "    for word in words:\r\n",
        "      if word not in stop_words:\r\n",
        "        s+=(word+' ')\r\n",
        "    s=s.strip()\r\n",
        "    new_sentences.append(s)\r\n",
        "  return new_sentences\r\n",
        "\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Ef2OAMccqI",
        "outputId": "48e07907-e9de-4000-9ade-eb9c0ede3656"
      },
      "source": [
        "with open('intents.json') as file:\r\n",
        "    data = json.load(file)\r\n",
        "\r\n",
        "\r\n",
        "training_sentences = []\r\n",
        "training_labels = []\r\n",
        "labels = []\r\n",
        "responses = []\r\n",
        "\r\n",
        "\r\n",
        "for intent in data['intents']:\r\n",
        "    for pattern in intent['patterns']:\r\n",
        "        training_sentences.append(pattern)\r\n",
        "        training_labels.append(intent['tag'])\r\n",
        "    responses.append(intent['responses'])\r\n",
        "    \r\n",
        "    if intent['tag'] not in labels:\r\n",
        "        labels.append(intent['tag'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#removing stopwords\r\n",
        "training_sentences = remove_stopwords(stop_words,training_sentences)\r\n",
        "\r\n",
        "\r\n",
        "print(\"after removing stopwords:\")\r\n",
        "print(\"new sentences : \",training_sentences)\r\n",
        "\r\n",
        "#encoding training labels\r\n",
        "lbl_encoder = label_encoder(training_labels) \r\n",
        "lbl_encoder.find_class_array()\r\n",
        "training_labels = lbl_encoder.Label_Encoder()\r\n",
        "\r\n",
        "\r\n",
        "print(\"after encoding : \",training_labels)\r\n",
        "\r\n",
        "\r\n",
        "vocab_size = 1000\r\n",
        "embedding_dim = 16\r\n",
        "max_len = 20\r\n",
        "oov_token = \"<OOV>\"\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) \r\n",
        "tokenizer.fit_on_texts(training_sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = texts_to_sequences(training_sentences,word_index)\r\n",
        "padded_sequences = pad_sequences(sequences,max_len)\r\n",
        "\r\n",
        "epochs = 550\r\n",
        "#history = model.fit(padded_sequences, np.array(training_labels), epochs=epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after removing stopwords:\n",
            "new sentences :  ['Hi', 'How', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'Bye', 'See later', 'Goodbye', 'Nice chatting you, bye', 'Till next time', 'Thanks', 'Thank', \"That's helpful\", 'Awesome, thanks', 'Thanks helping', 'How could help me?', 'What do?', 'What help provide?', 'How helpful?', 'What support offered', 'How check Adverse drug reaction?', 'Open adverse drugs module', 'Give list drugs causing adverse behavior', 'List drugs suitable patient adverse reaction', 'Which drugs dont adverse reaction?', 'Open blood pressure module', 'Task related blood pressure', 'Blood pressure data entry', 'I want log blood pressure results', 'Blood pressure data management', 'I want search blood pressure result history', 'Blood pressure patient', 'Load patient blood pressure result', 'Show blood pressure results patient', 'Find blood pressure results ID', 'Find pharmacy', 'Find pharmacy', 'List pharmacies nearby', 'Locate pharmacy', 'Search pharmacy', 'Lookup hospital', 'Searching hospital transfer patient', 'I want search hospital data', 'Hospital lookup patient', 'Looking hospital details']\n",
            "after encoding :  [4 4 4 4 4 4 4 3 3 3 3 3 8 8 8 8 8 6 6 6 6 6 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2\n",
            " 7 7 7 7 7 5 5 5 5 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaRLOS-h4o0N"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58e5zc3l1Wsf",
        "outputId": "67f4847d-f60e-4c8d-ff66-09615c6b878c"
      },
      "source": [
        "print(word_index)\r\n",
        "word_index = dict(sorted(word_index.items()))\r\n",
        "#word_index"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'blood': 2, 'pressure': 3, 'patient': 4, 'adverse': 5, 'hospital': 6, 'how': 7, 'drugs': 8, 'pharmacy': 9, 'thanks': 10, 'what': 11, 'reaction': 12, 'list': 13, 'data': 14, 'i': 15, 'want': 16, 'results': 17, 'search': 18, 'find': 19, 'bye': 20, 'helpful': 21, 'help': 22, 'open': 23, 'module': 24, 'result': 25, 'lookup': 26, 'hi': 27, 'is': 28, 'anyone': 29, 'there': 30, 'hey': 31, 'hola': 32, 'hello': 33, 'good': 34, 'day': 35, 'see': 36, 'later': 37, 'goodbye': 38, 'nice': 39, 'chatting': 40, 'you': 41, 'till': 42, 'next': 43, 'time': 44, 'thank': 45, \"that's\": 46, 'awesome': 47, 'helping': 48, 'could': 49, 'me': 50, 'do': 51, 'provide': 52, 'support': 53, 'offered': 54, 'check': 55, 'drug': 56, 'give': 57, 'causing': 58, 'behavior': 59, 'suitable': 60, 'which': 61, 'dont': 62, 'task': 63, 'related': 64, 'entry': 65, 'log': 66, 'management': 67, 'history': 68, 'load': 69, 'show': 70, 'id': 71, 'pharmacies': 72, 'nearby': 73, 'locate': 74, 'searching': 75, 'transfer': 76, 'looking': 77, 'details': 78}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab1GOA8-fah",
        "outputId": "1ae8305f-14d2-4933-e4a9-d505e47dbc2c"
      },
      "source": [
        "seq = texts_to_sequences(training_sentences,word_index)\r\n",
        "print(seq)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27], [7], [28, 29, 30], [31], [32], [33], [34, 35], [20], [36, 37], [38], [39, 40, 41, 20], [42, 43, 44], [10], [45], [46, 21], [47, 10], [10, 48], [7, 49, 22, 50], [11, 51], [11, 22, 52], [7, 21], [11, 53, 54], [7, 55, 5, 56, 12], [23, 5, 8, 24], [57, 13, 8, 58, 5, 59], [13, 8, 60, 4, 5, 12], [61, 8, 62, 5, 12], [23, 2, 3, 24], [63, 64, 2, 3], [2, 3, 14, 65], [15, 16, 66, 2, 3, 17], [2, 3, 14, 67], [15, 16, 18, 2, 3, 25, 68], [2, 3, 4], [69, 4, 2, 3, 25], [70, 2, 3, 17, 4], [19, 2, 3, 17, 71], [19, 9], [19, 9], [13, 72, 73], [74, 9], [18, 9], [26, 6], [75, 6, 76, 4], [15, 16, 18, 6, 14], [6, 26, 4], [77, 6, 78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDARORm9G_Wr",
        "outputId": "8ee9b72b-1172-4e85-c5ca-87a0128baddc"
      },
      "source": [
        "print(padded_sequences)\r\n",
        "#padded_sequences2[0,:].shape\r\n",
        "# if seq == sequences:\r\n",
        "#   print('true')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 29 30]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 31]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34 35]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 36 37]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 38]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 39 40 41 20]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 42 43 44]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 45]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 46 21]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 47 10]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 48]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 49 22 50]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11 51]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11 22 52]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 21]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 11 53 54]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7 55  5 56 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  5  8 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 57 13  8 58  5 59]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 13  8 60  4  5 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  8 62  5 12]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 23  2  3 24]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 63 64  2  3]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 14 65]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 16 66  2  3 17]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3 14 67]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 15 16 18  2  3 25 68]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 69  4  2  3 25]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 70  2  3 17  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  2  3 17 71]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 13 72 73]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 74  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 18  9]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26  6]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 75  6 76  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15 16 18  6 14]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6 26  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 77  6 78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0WCIRSZ7z7"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKNik1r9n6a"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMW5ZNXU9-kX"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWC20srd-3_T"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol3pdZ9D_KVf"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8p4cPJYCgQQ"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}